https://web.njit.edu/~egan/Writing_A_Literature_Review.pdf									(Literatur Review)

ARCHITECTURES
https://arxiv.org/pdf/1302.4389.pdf												(MaxOutNetworks)
https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf							(HAN)
https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf										(Tensor Tree RNN)
https://www.bioinf.jku.at/publications/older/ch7.pdf										(Vanishing Gradient Problem RNN)
http://www.bioinf.jku.at/publications/older/2604.pdf										(LSTM)
https://arxiv.org/pdf/1409.1259.pdf												(GRU)
https://ieeexplore.ieee.org/document/650093											(Bi RNN)
http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf						(Seq2Seq)
http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf										(CNN)
https://arxiv.org/pdf/1512.00567.pdf	|	https://arxiv.org/pdf/1512.03385.pdf						(bottleneck layer)
https://www.bioinf.jku.at/publications/older/ch7.pdf										(Vanishing Gradient Problem CNN)
https://arxiv.org/pdf/1505.00387.pdf												(Highway Networks)
https://arxiv.org/pdf/1512.03385.pdf												(Residual Networks)
https://arxiv.org/pdf/1608.06993.pdf												(DenseNets)
http://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf								(Capsule Networks)
https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf			(Inception Networks)
https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745/9552								(RCNN)
http://openaccess.thecvf.com/content_cvpr_2015/papers/Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper.pdf			(RCNN)
https://arxiv.org/pdf/1409.0473.pdf												(Attention => Anwendungsbeispiel falls das original paper nicht reicht https://arxiv.org/pdf/1508.04025.pdf)
https://arxiv.org/abs/1703.03130												(self Attention)
https://pypi.org/project/keras-transformer/											(Transformer Code)
http://nlp.seas.harvard.edu/2018/04/03/attention.html										(Transformers: Erklärung des Papers Attention is all you need, unter https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)

EMBEDDING/PRETRAINING:
https://www.aclweb.org/anthology/P12-2018											(Bag Of Words)
https://www.aclweb.org/anthology/P10-1040.pdf											(Word Representation)
https://pdfs.semanticscholar.org/3fc9/7768dc0b36449ec377d6a4cad8827908d5b4.pdf							(Sentence Embedding)
https://arxiv.org/pdf/1301.3781.pdf												(Word2Vec vorgänger)
https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf		(Word2Vec)
https://arxiv.org/pdf/1607.01759v3.pdf												(FastText)
http://proceedings.mlr.press/v32/le14.pdf											(Doc2Vec)
https://arxiv.org/abs/1902.06006												(Contextual Word Vectors)
http://www.aclweb.org/anthology/D14-1162											(GloVe)
https://arxiv.org/pdf/1708.00107.pdf												(CoVe)
https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf								(GPT)
https://paperswithcode.com/paper/language-models-are-unsupervised-multitask							(GPT-2)
https://aclweb.org/anthology/N18-1202												(ELMO)
https://arxiv.org/abs/1810.04805												(BERT)
https://arxiv.org/abs/1801.06146												(ULMFit)
https://arxiv.org/abs/1906.08237												(XLNet)

HACKYSTUFF:
https://arxiv.org/pdf/1703.03906.pdf												(Hyper Params)
https://arxiv.org/pdf/1607.06450.pdf												(Layer Normalization)
https://arxiv.org/abs/1707.09861												(Random Init => local Minima)
https://arxiv.org/abs/1707.06799												(Hyperparmas)
http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf								(Dropout macht alles besser)
https://arxiv.org/abs/1412.6980													(Adam Optimizer)
http://proceedings.mlr.press/v37/ioffe15.html											(Batchnormalization)

GENERAL:
https://www.cs.cmu.edu/~epxing/Class/10715/reading/Kornick_et_al.pdf								(Universal Function Approximators für bestimmte arten von fkt und ggf für alle fkt https://papers.nips.cc/paper/5422-on-the-number-of-linear-regions-of-deep-neural-networks.pdf)
https://jhu.pure.elsevier.com/en/publications/catastrophic-interference-in-connectionist-networks-the-sequentia-4		(Catastrophic forgetting)
https://arxiv.org/pdf/1412.6572.pdf												(Adverserial Training)
http://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedding.pdf							(Word Embedding Dimensionality, most common is 300, but best case would be to explore best embedding)
http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf										(BackProp)
https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf										(ReLu)
https://arxiv.org/abs/1606.08415												(GELU)
https://openreview.net/pdf?id=rJ4km2R5t7											(GLUE Benchmark)
https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf								(Transfer Learning)
Francois Chollet. 2015. Keras.https://github.com/fchollet/keras									(Keras)

MISC.:
https://arxiv.org/ftp/arxiv/papers/1907/1907.02581.pdf										(Transfer Learning Can Enhance Text Classification)
https://research.fb.com/building-an-efficient-neural-language-model-over-a-billion-words/ => perplexity vgl von papern		(RNN)
https://guillaumegenthial.github.io/												(Code Examples)

