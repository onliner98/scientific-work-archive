\chapter{Proposed Model}
\label{ch:model}
This chapter discusses which model satisfies the requirements listed in Chapter \ref{ch:requirements} the most. The requirements are discussed individually. In order to keep it clear which requirement is discussed the requirement is referenced using its key specified in Chapter \ref{ch:requirements} followed by a short summary of the requirement. Based on this discussion a model for the recruitment advertisement classification task is proposed. Subsequently, the proposed model is described in detail. 
\section{Discussion}
\label{sec:discussion}
\textbf{Task} requires a neural model for text classification. The state of the art on these models is analyzed by the conducted literature review. The literature review finds \ac{CNN}s, \ac{RNN}s, Ensembles and Transformers to be considered for the proposed model. All models found are listed in Chapter \ref{ch:sota}. Among these models, Transformers achieve the highest average performance on the GLUE\footnote{\url{https://gluebenchmark.com/leaderboard/}}, NLP Progress\footnote{\url{https://nlpprogress.com/english/text_classification.html}} and Papers With Code\footnote{\url{https://paperswithcode.com/task/text-classification}} leaderboards. According to these leaderboards, XLNet\autocite{Yang.2019} is the on average best performing Transformer, followed by BERT\autocite{Devlin.2018}. The current rank one of the GLUE\footnote{\url{https://gluebenchmark.com/leaderboard/}} leaderboard is BERT's successor ALBERT\autocite{Lan.2019}. However, the paper on ALBERT was just released September 2019 and is on conference submission to the ICLR 2020. Hence, as off now there is not much data available on ALBERT's performance for classification tasks. As a result, XLNet seems to be the model of choice if it satisfies the other requirements.
\par
\textbf{Input} requires the model's input to be structured as plain text. The input of XLNet is structured as a sequence of tokens.\autocite{Yang.2019} A plain text is converted to a sequence of tokens by a tokenizer. Consequently, XLNet complies with the restrictions imposed by \textbf{Input}. The same applies to BERT and ALBERT. However, XLNet has an edge over BERT and ALBERT. The sequence length of BERT and ALBERT is limited.\autocites{Devlin.2018}{Lan.2019} On that account, recruitment advertisements longer than the specified sequence length are truncated. This is not the case for XLNet.\autocite{Yang.2019} In general, plain text can be converted to comply the input restrictions of any state of the art model.
\par
\textbf{Output} requires the model to return the vocation id and the standardized job title corresponding to the input. Recruitment advertisement classification is a multi-class classification. Accordingly, the output of the model is a vector of dimensionality $K$ equal to the number of classes. Each dimension corresponds to a class.\autocites{Devlin.2018}{Yang.2019}{Lan.2019} For further explanation, see Section \ref{sec:classification}.  Due to that, an index mapping can be used between this vector and arrays containing the vocation id and standardized job title. Hence, any state of the art model can be used to satisfy the \textbf{Output} requirement.
\par
\textbf{Training Data} describes the available training data. The training data is not structured as required by \textbf{Input}. Because of this, the training data needs to be converted. This is done by a parser. Since the training data is labeled using the Stepstone \ac{API}'s search algorithm, the data is most likely to be noisy. For this reason, the use of a match rank threshold should be examined. The quality of the training data can be further improved by human labeling, but the costs increase with the size of the dataset. However, for a fine-tuning approach a small dataset is sufficient. Derived from this, any state of the art model can be used to satisfy the \textbf{Training Data} requirement.
\par
\textbf{Dataset size} leads to the use of pre-training and fine-tuning, as specified in Section \ref{sec:learning}, since the amount of usable training data is uncertain and the creation of high quality training data through human labeling is expensive. A pre-training and fine-tuning method is defined for XLNet, BERT and ALBERT. All three models use some kind of language model pre-training. In addition, BERT and ALBERT use a kind of sentence order objective.\autocites{Devlin.2018}{Yang.2019}{Lan.2019} Following, all three models can be used to satisfy the \textbf{Dataset size} requirement. In general, pre-training and fine-tuning can be used for any state of the art model.
\par
\textbf{Infrastructure} requires the use of \ac{AWS}. The \ac{AWS} instance g4dn.xlarge has 16 GB GPU memory.\footnote{\url{https://aws.amazon.com/ec2/instance-types/?nc1=h_ls}} This is sufficient for any state of the art model, even for the lager models XLNet, BERT and ALBERT.\footnote{\url{https://github.com/zihangdai/xlnet/blob/master/README.md}}\footnote{\url{https://github.com/google-research/bert}}\autocite{Lan.2019}
\par
\textbf{Costs} requires a models with similar performance but cost advantages over another model to be preferred. XLNet and BERT not only outperform other models, they are also available already implemented and pre-trained under Apache License 2.0.\footnote{\url{https://github.com/zihangdai/xlnet/blob/master/LICENSE}}\footnote{\url{https://github.com/google-research/bert/blob/master/LICENSE}}
Implementation and pre-training is expensive.\autocites{Devlin.2018}{Yang.2019}{Lan.2019} Apache License 2.0 allows for commercial use free of charge. This results in performance per cost advantages for XLNet and BERT.
\par
In general, any state of the art model for text classification satisfies the requirements. However, mainly due to performance reasons and performance per cost advantages XLNet seems to be the best choice. Therefore, this paper proposes the pre-trained XLNet fine-tuned on the recruitment advertisement classification task as optimal model for this task.



\section{XLNet}
XLNet is an adapted Transformer model. The adaption is necessary, because of XLNet's novel pre-training objective. On that account, the pre-training objective is explained in order to properly understand XLNet's architecture. According to \cite{Yang.2019}, the most successful pre-training objectives are autoregressive language modeling and autoencoding. Both methods have an advantage complementary to the disadvantage of the other. Autoregressive language modeling predicts the next word in a sequence. Thus, only words at one end of the sequence can be predicted. Accordingly, autoregressive language modeling is only depended on left or right context depending on the direction of prediction. For example, given the sequence \enquote{New York is a city}, a left to right autoregressive language model could predict \enquote{city} from \enquote{New York is a}, but not \enquote{is} from \enquote{New York, a city}. This method could only predict \enquote{is} from \enquote{New York}. Consequently, the context on the right is lost.\autocite{Yang.2019}
\par
Autoencoding predicts masked words in a sequence. Hence, words anywhere in the sequence can be predicted, but if several words are masked the context of these words is lost. For example, given the sequence \enquote{New York is a city} with \enquote{is} and \enquote{a} masked, both masked words are predicted from \enquote{New York, city}. Even though due to the sequence of words \enquote{a} should be predicted from \enquote{New York is, city}. As a result, sequence context is lost.\autocite{Yang.2019}
\par
Summarizing, autoregressive language modeling preserves sequence context while losing left or right context, and autoencoding preserves left and right context while losing sequence context. To overcome the disadvantages while preserving the advantages, XLNet's objective is to predict masked words anywhere in the sentence from the whole sequence context including all masked words previous to the predicted word. For example, given the sequence \enquote{New York is a city} with \enquote{New} and \enquote{York} masked, \enquote{New} is predicted from \enquote{is a city} and \enquote{York} is predicted from \enquote{New, is a city}.\autocite{Yang.2019}
\par
XLNet achieves this by providing the complete input sequence. However, predicting a masked word given the whole sequence including the masked word is trivial. For the prediction, the hidden state used to predict the masked word should only be aware of the masked words context and not the masked word itself.\footnote{Words in the context of XLNet are actually byte pair encoded word pieces with positional information embedded in a vector space.} For this reason, XLNet uses two hidden states, as described in Equation \eqref{eq:xlnet}.\autocite{Yang.2019}
\begin{itemize}
	\item  The content representation $h_{z_t}$ encoding the context of the input and the input $x_{z_t}$ itself. This representation serves a similar role to the  hidden states in a vanilla Transformer.
	\item The query representation $g_{z_t}$ only encoding the context $x_{z < t}$ and the position $z_t$, but not the input $x_{z_t}$ itself as discussed above. 
\end{itemize}
\begin{equation}
\label{eq:xlnet}
	\begin{array}{lcr}
		g_{z_t}^{(m)} & \gets & Attention(Q=g_{z_t}^{(m-1)},KV=h_{z < t}^{(m-1)}; \theta)\\
		h_{z_t}^{(m)} & \gets & Attention(Q=h_{z_t}^{(m-1)},KV=h_{z \le t}^{(m-1)}; \theta)\\
	\end{array}
\end{equation}
The query representation is aware of the complete context, but not the input itself. Due to that, it satisfies the requirements of the pre-training objective. The content representation is calculated the same way as in a vanilla Transformer. Therefore, during fine-tuning, the query stream is dropped and the content stream is used as in a normal Transformer.\autocite{Yang.2019}
\par
In addition to the query stream XLNet has two more adaptions compared to a vanilla Transformer a relative positional encoding scheme and a segment recurrence mechanism. The relative positional encoding scheme as defined by \cite{Dai.2019} is used to replace the vanilla Transformer's position encoding. The segment recurrence mechanism serves as memory to enable an unrestricted sequence input. The input sequence of a the vanilla Transformer is restricted to length $T$. The segment recurrence mechanism is defined as follows. \blockcquote{Yang.2019}{Without loss of generality, suppose we have two segments taken from a long sequence $s$; i.e., $\tilde{x} = s_{1:T}$  and $x = s_{T+1:2T}$. [...] We process the first segment, and then cache the obtained content representations $\tilde{h}^{(m)}$ for each layer $m$.} Then for the next segment $x$, the attention update with memory can be written as in Equation \eqref{eq:memory}.\autocite{Yang.2019}
\begin{equation}
	\label{eq:memory}
	h_{z_t}^{(m)} \gets Attention(Q=h_{z_t}^{(m-1)},KV=[\tilde{h}^{(m-1)}, h_{z \le t}^{(m-1)]}; \theta)
\end{equation}
\par
For fine-tuning, XLNet is first initialized with the pre-trained parameters. These parameters are fine-tuned by using the labeled data of the fine-tuning task. Recalling, the task of the proposed model is to classify recruitment advertisements. XLNet is fine-tuned for a classification task by applying a softmax classifier to the last hidden state in the output sequence of the last Transformer Block. The softmax classifier is a \ac{MLP} with one hidden layer and as many outputs as the number of classes. Hence, its output is a class distribution vector with each scalar representing the probability of the corresponding class. For further explanation, see Section \ref{sec:classification}. Given the last hidden state $h_T^{(N)}$ and the weight matrix of the softmax classifier $W_{K \times H}$, where $N$ is the number of Transformer Blocks, $T$ is the length of the sequence, $K$ is the number of classes and $H$ is the number of the hidden vector's dimensions, the softmax classifier is described in Equation \eqref{eq:finetune}.\autocites{Yang.2019}{Devlin.2018}
\begin{equation}
	\label{eq:finetune}
	softmax(W_{K \times H}h_T^{(N)})
\end{equation}
In regard to the recruitment advertisement classification task, the labeled data of the fine-tuning task refers to the tokenized job title followed by the job description. Since the training data as provided by Aivy is structured in XML, it needs to be fitted to the input structure of XLNet. This is done by a parser and a tokenizer. The class distribution vector is derived from the vocation ids, with one dimension corresponding to one vocation id.
\par
After fine-tuning, the parameters of the model are frozen. To be classified, a recruitment advertisement needs to comply to XLNet's input structure. On that account, its job title and job description are stripped of their structure and tokenized. Subsequently, this input is fed into the fine-tuned XLNet. This results in the class distribution vector of that input. Since each dimension of the class distribution vector corresponds to a vocation id, the argmax of the vector can be mapped directly to a vocation id and a standardized job title. These are the model's prediction for that recruitment advertisement.