Recapitulating, a neural network can be seen as a universal function approximator.\autocite{Ertel.2016} A loss function is a metric that measures how well a neural network approximates a function. The resulting value is called loss $\mathcal{L}$ \autocite{ElAmir.2020}. For example, if a function classifying tool images is to be approximated, the loss function measures how well the neural network classifies the tool images.
\par
Neural networks are trained by reducing the loss. The loss is reduced by adapting the parameters $\theta$ of the neural network in such a way that $\mathcal{L}$ is minimized. To properly adjust $\theta$, backpropagation \autocite{Rumelhart.1986} is used to calculate a gradient vector $\nabla {\theta}$. This gradient indicates by which amount the error increases or decreases if $\theta$ is increased by a very small amount. The loss function in regard to $\theta$ can be seen as high-dimensional hilly landscape, with the negative gradient vector indicating the direction of the steepest descent in that landscape. The gradient is used to descend in that landscape to find a local or the global minimum. \autocite{LeCun.2015} Accordingly, training a neural network can be seen as solving an optimization problem. \autocite{ElAmir.2020}  